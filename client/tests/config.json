{"@type": "InferenceConfiguration", "pipelineSteps": [{"@type": "ModelPipelineStep", "inputNames": ["IteratorGetNext:0", "IteratorGetNext:1", "IteratorGetNext:4"], "outputNames": ["loss/Softmax"], "modelConfig": {"@type": "TensorFlowConfig", "tensorDataTypesConfig": {"@type": "TensorDataTypesConfig", "inputDataTypes": {"IteratorGetNext:0": "INT32", "IteratorGetNext:1": "INT32", "IteratorGetNext:4": "INT32"}}, "modelConfigType": {"@type": "ModelConfigType", "modelType": "TENSORFLOW", "modelLoadingPath": "bert_mrpc_frozen.pb"}}, "servingConfig": {"@type": "ServingConfig", "httpPort": 34762, "inputDataType": "NUMPY", "outputDataType": "NUMPY", "parallelInferenceConfig": {"@type": "ParallelInferenceConfig", "workers": 1}, "logTimings": true}}], "servingConfig": {"@type": "ServingConfig", "httpPort": 34762, "inputDataType": "NUMPY", "outputDataType": "NUMPY", "parallelInferenceConfig": {"@type": "ParallelInferenceConfig", "workers": 1}, "logTimings": true}}